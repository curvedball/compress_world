数据能够压缩的基础：
有重复模式
符号的分布不均匀；
残值的取值范围较小；



1.构建连续数据法
插入了不同的端口值，影响了后续的查找和编码，例如端口号
c1 c1
c1 c2
c1 c3
c1 c4
2a 83
c1 c5
ba d4
c1 c6
f0 55
c1 c7
...
思路: 
端口号根据实际情况与它的上一个值做delta编码，并且用一个额外的control_bit比特数组来记录。如果需要做delta编码,则control_bit设置为1,否则设置为0。对于插入的奇异值，可以设法保存到control_bit数组中去, 数值的宽度可以事先约定。最终，可以提取出连续的数值。对连续的数值进行处理，压缩效果更好。

判断delta编码的依据: 
统计各个符号出现的频率。如果delta编码后，出现次数最大的符号发生了改变或者根据信息论的理解，差分编码后熵减少了，则也可以进行差分编码。





2. 缩小范围法
对于数值型的字段，x-u之后可以减少x的取值范围，其中u是随机变量x的均值。
更小的数值范围可以用更少的比特表示，少量取值范围大的那些数值可以通过另外一个control_bit数组来记录，类似zstd算法中的LL_base数组。


3. AI方法
通过大量的数据进行训练，把一个字段的index值看成是横坐标，实际值看成是纵坐标，然后通过机器学习，找到线性或者非线性函数关系，
然后将实际值进行正则化处理，即转换为与预测值的残值。这样更容易实现压缩。


4.　膨胀压缩法　
类似于SVM算法中的核函数(先提高维度)，先插入一些数据让原始数据膨胀，使得它符合某个模式，再压缩


5. 分离法
将一段连续的数据转换为两个连续的数据，其中一个存放随机数据，另外一个存放有特点的数据，然后通过一个bitmap来记录数据分离情况。
最后，只是对有特点的数据进行压缩，因为随机数据是没有办法压缩的。


6.重新排列法
把记录分解为各个字段，每个字段的数据放在一起，具有最大的相似性，因此容易出现重复模式；
有没有更普通的排列方法呢？


7.bit重排法
使用bwt变换，将一个字节内部的比特重新排序，然后建立一个字典来标记这种排序结果，能缩减为3bit或者4bit来表示？




8. 长度变换法
长度变换法，LL_code数组主动构造了一些区间，并不能做到最优。
能否有更合适的长度变换法呢?


9. 字节的比特查表法
重新排列00000000~11111111的顺序，使得连续增加的数值尽量具有相同的前缀(７个相同前缀的放在一起，然后是６个相同前缀的放在一起，依次类推)。
这样对于不断增加的数值，可以得到尽可能多的前缀，再按照列进行压缩。



10. 插值预测法
无论函数关系如何，采用插值预测法总是可以得到一个小范围的残值。首尾各保留２个字节。
尾部是否含有0.5需要进行区分，解决办法是残值乘以２，将来计算时再除以２就可以恢复。
异常值的残值会非常大，不进行处理。对于端口号之类的值，引入4个bit来表示残值的位宽和残值表示。
这样就可以根据前２个点的真值与残值推导出第三个点的真值，再根据第二个点和第三个点的真值与残值可以推导出第４个点，依次类推。
仍然需要排除异常值（通过一个bitmap来记录，残值放在一起，原始值放在一起）。
X=(X1+X2)/2相当于(x-x1)~(x2-x)，因此找出差值出现次数最多的值作为模式, 这种方式有误，比如3,4,10,11,100,101，它们的差值也是１，但是没有规律。
可以先做差值，然后拿差值对应的实际值序列来计算均值和方差，从而检测序列是否有规律。如果有规律，则保留有规律的这些值，排除其他的异常值。
考虑模式不会贯通整个文件，因此需要分成各个小块进行处理。
“要找出聚类，可以依次后面的值减去前面的值，如果连续的值都位于delta_threshold范围内，则它们对应的值是一个模式。”
实际运用时，可以监视压缩比，如果太小，则走该算法进行压缩，然后与默认压缩进行比较。






















