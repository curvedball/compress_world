数据能够压缩的基础：
有重复模式
符号的分布不均匀；
残值的取值范围较小；



1.构建连续数据法
插入了不同的端口值，影响了后续的查找和编码，例如端口号
c1 c1
c1 c2
c1 c3
c1 c4
2a 83
c1 c5
ba d4
c1 c6
f0 55
c1 c7
...
思路: 
端口号根据实际情况与它的上一个值做delta编码，并且用一个额外的control_bit比特数组来记录。如果需要做delta编码,则control_bit设置为1,否则设置为0。对于插入的奇异值，可以设法保存到control_bit数组中去, 数值的宽度可以事先约定。最终，可以提取出连续的数值。对连续的数值进行处理，压缩效果更好。

判断delta编码的依据: 
统计各个符号出现的频率。如果delta编码后，出现次数最大的符号发生了改变或者根据信息论的理解，差分编码后熵减少了，则也可以进行差分编码。





2. 缩小范围法
对于数值型的字段，x-u之后可以减少x的取值范围，其中u是随机变量x的均值。
更小的数值范围可以用更少的比特表示，少量取值范围大的那些数值可以通过另外一个control_bit数组来记录，类似zstd算法中的LL_base数组。


3. AI方法
通过大量的数据进行训练，把一个字段的index值看成是横坐标，实际值看成是纵坐标，然后通过机器学习，找到线性或者非线性函数关系，
然后将实际值进行正则化处理，即转换为与预测值的残值。这样更容易实现压缩。


4.　膨胀压缩法　
类似于SVM算法中的核函数(先提高维度)，先插入一些数据让原始数据膨胀，使得它符合某个模式，再压缩


5. 分离法
将一段连续的数据转换为两个连续的数据，其中一个存放随机数据，另外一个存放有特点的数据，然后通过一个bitmap来记录数据分离情况。
最后，只是对有特点的数据进行压缩，因为随机数据是没有办法压缩的。


6.重新排列法
把记录分解为各个字段，每个字段的数据放在一起，具有最大的相似性，因此容易出现重复模式；
有没有更普通的排列方法呢？


7.bit重排法
使用bwt变换，将一个字节内部的比特重新排序，然后建立一个字典来标记这种排序结果，能缩减为3bit或者4bit来表示？




8. 扩展差分编码+长度变换法
长度变换法，LL_code数组主动构造了一些区间，并不能做到最优。假设对端口号操作，构造a[256]数组和b[256]数组，里面放入一些边界点，
原始文件的数据点经过转换后输出的是a[256]数组的下标和需要保存的补充比特；
只要约定的a[256]数组的值是确定的，b[256]数组内的比特长度值是确定的，就能够解码。
这样可以将16bit的端口号尽量压缩成以9比特为主（仍然要想办法去除index对应的8个bit）。




9. 字节的比特查表法
重新排列00000000~11111111的顺序，使得连续增加的数值尽量具有相同的前缀(７个相同前缀的放在一起，然后是６个相同前缀的放在一起，依次类推)。
这样对于不断增加的数值，可以得到尽可能多的前缀，再按照列进行压缩。



10. 插值预测法
无论函数关系如何，采用插值预测法总是可以得到一个小范围的残值。首尾各保留２个字节。
尾部是否含有0.5需要进行区分，解决办法是残值乘以２，将来计算时再除以２就可以恢复。
异常值的残值会非常大，不进行处理。对于端口号之类的值，引入4个bit来表示残值的位宽和残值表示。
这样就可以根据前２个点的真值与残值推导出第三个点的真值，再根据第二个点和第三个点的真值与残值可以推导出第４个点，依次类推。
仍然需要排除异常值（通过一个bitmap来记录，残值放在一起，原始值放在一起）。
X=(X1+X2)/2相当于(x-x1)~(x2-x)，因此找出差值出现次数最多的值作为模式, 这种方式有误，比如3,4,10,11,100,101，它们的差值也是１，但是没有规律。
可以先做差值，然后拿差值对应的实际值序列来计算均值和方差，从而检测序列是否有规律。如果有规律，则保留有规律的这些值，排除其他的异常值。
考虑模式不会贯通整个文件，因此需要分成各个小块进行处理。
“要找出聚类，可以依次后面的值减去前面的值，如果连续的值都位于delta_threshold范围内，则它们对应的值是一个模式。仍然有ｂｕｇ”

“选择６４个输入数据点，设计一个滑动窗口，滑动窗口选为这些点的边界开始，观察滑动窗口内是否有多个数值存在，这些数值称为标准值，计算它们的均值和方差, 这样就找到了模式”
“不同的段的模式最后都放在一起，形成模式数据，其他数据隔离出来”，这个方式仍有ｂｕｇ，模式连接太模糊了，太困难了。

分成32个点进行排序（需要记录排序前的位置，也不可行）, 如果有超过1/2的点满足关系式

实际运用时，可以监视压缩比，如果太小，则走该算法进行压缩，然后与默认压缩进行比较。




11. 相似性度量法
抽取出奇序列和偶序列，然后奇数它们之间的差值，如果在delta_threshold范围内的值超过50%，则认为数据是有规律的。



12. 参考文件法
原始数据做相邻的差值运算，统计delta_threshold范围内的各个值的出现次数；
如果有某个符号出现次数超过25%，则认为该文件中存在该模式；
差值是该符号的对应数值是好点，作为参考文件的一部分数据；
对好点之外的空闲空间的值进行识别，不断补齐对应的点, 仍然有ｂｕｇ；
不能补齐的点作为异常值剩余下来。


13. Huffman+Delta
能够使用Huffman来对ｄｅｌｔａ进行编码吗？




















